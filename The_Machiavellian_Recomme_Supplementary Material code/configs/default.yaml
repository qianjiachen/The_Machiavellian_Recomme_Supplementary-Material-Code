# Default configuration for Machiavellian Recommender experiments

# Arena configuration
arena:
  num_users: 1000
  num_vendors: 100
  num_items_per_vendor: 50
  communication_budget: 5
  max_tokens: 128
  discount_factor: 0.99
  embedding_dim: 128

# SWA training configuration
swa:
  lambda_welfare: 0.5
  lambda_equity: 0.3
  learning_rate: 3.0e-4
  batch_size: 256
  num_steps: 500000
  warmup_steps: 10000
  kl_coef: 0.01
  gradient_clip_norm: 1.0
  target_update_tau: 0.005

# LLM configuration
llm:
  model_name: "meta-llama/Llama-3-8b-hf"  # Use smaller model for testing
  lora_rank: 16
  lora_alpha: 32
  lora_dropout: 0.1
  device: "cuda"
  dtype: "float16"

# Detection configuration
detection:
  num_paraphrases: 5
  threshold_tau: 0.3
  threshold_mi: 0.1
  mine_hidden_dim: 128
  mine_num_layers: 3

# Evaluation configuration
evaluation:
  num_seeds: 5
  eval_interval: 10000
  checkpoint_interval: 50000

# Logging configuration
logging:
  use_tensorboard: true
  use_wandb: false
  log_interval: 1000
  save_dir: "outputs"
